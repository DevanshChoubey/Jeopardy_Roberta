{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n%config Completer.use_jedi = False","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/imdb-review-dataset/imdb_master.csv\n/kaggle/input/200000-jeopardy-questions/JEOPARDY_CSV.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!wget https://files.consumerfinance.gov/ccdb/complaints.csv.zip\n!unzip ./complaints.csv.zip\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport transformers\n#!pip install Tez\n#import tez\nimport torch.nn as nn\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom sklearn import metrics\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BERTDataset(Dataset):\n    def __init__(self, texts, targets, max_len = 64):\n        self.texts = texts\n        self.targets = targets\n        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n                        \"bert-base-uncased\", do_lower_case = True\n        )\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        inputs = self.tokenizer.encode_plus(\n                text,\n                None,\n                add_special_tokens = True,\n                max_length = self.max_len,\n                padding = \"max_length\",\n                truncation = True\n        )\n        resp = {\n            \"ids\" : torch.tensor(inputs[\"input_ids\"], dtype=torch.long),\n            \"mask\" : torch.tensor(inputs[\"attention_mask\"], dtype=torch.long),\n            \"token_type_ids\" : torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long),\n            \"targets\" : torch.tensor(self.targets[\"idx\"], dtype = torch.float)\n        }\n        return resp\n    \n    \nclass TextModel(tez.Model):\n    def __init__(self, num_classes, num_train_steps):\n        super().__init__()\n        self.bert = transformers.BertModel.from_pretrained(\n                    \"bert-base-uncased\", return_dict = False\n        )\n        self.bert_drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, num_classes)\n        self.num_train_steps = num_train_steps\n        self.optim_ = self.fetch_optimizer()\n        \n    def fetch_optimizer(self):\n        opt = AdamW(self.parameters(), lr = 1e-4)\n        \n    def fetch_scheduler(self):\n        sch = get_linear_schedule_with_warmup(\n            AdamW(self.parameters(), lr = 1e-4), num_warmup_steps = 0,num_training_steps = self.num_train_steps\n        )\n        return sch\n    \n    \n    def loss(self, outputs, targets):\n        return nn.BCEWithLogitsLoss()(outputs, targets.view(-1,1))\n    \n    def monitor_metrics(self, outputs, targets):\n        outputs = torch.sigmoid(outputs).cpu().detach().numpy() >= 0.5\n        targets = targets.cpu().detach().numpy()\n        return {\n            \"accuracy\" : metrics.accruacy_score(targets, outputs)\n        }\n    \n    def forward(self, ids, mark, token_type_ids, targets = None):\n        _, x = self.bert(ids, attention_mask = mask, token_type_ids = token_type_ids)\n        x = self.bert_drop(x)\n        x = self.out(x)\n        if(targets is not None):\n            loss = self.loss(outputs , targets)\n            metrics = self.monitor_metrics(x , targets)\n            return x, loss, met\n        return x, 0, {}\n    \n\n    def train_model(out1,out2):\n        \n        \n        train_dataset = BERTDataset(out1.Title.values, out1.label.values)\n        valid_dataset = BERTDataset(out2.Title.values, out2.label.values)\n        \n        n_train_steps = int(len(out1) / 32 * 10)\n        \n        model = TextModel(num_classes = 1, num_train_steps = n_train_steps, )\n        \n        ess = tez.callbacks.EarlyStopping(monitor = \"valid_loss\", patience = 3, model_path = \"model.bin\")\n    \n        model.fit(\n                train_dataset,\n                valid_dataset = valid_dataset,\n                device = \"cuda\",\n                epochs = 10,\n                train_bs = 32,\n                callbacks = [ess]\n        )\n    \n    \n\n    \n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#!wget https://raw.githubusercontent.com/susanli2016/NLP-with-Python/master/data/title_conference.csv\n!wget https://raw.githubusercontent.com/susanli2016/NLP-with-Python/master/data/title_conference.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/200000-jeopardy-questions/JEOPARDY_CSV.csv').fillna(\"none\")\n#df['Conference'].value_counts()\n# possible_label = df[\" Value\"].unique()\n# label_dict = {}\n# for index, possible_label in enumerate(possible_label):\n#     label_dict[possible_label] = index\n\n# df['label'] = df[\" Value\"].replace(label_dict)\n# #df.head()","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head\ndf.isnull().sum()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"Show Number    0\n Air Date      0\n Round         0\n Category      0\n Value         0\n Question      0\n Answer        0\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import seaborn as sns\nsns.countplot(x=' Round',data=df)","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"<AxesSubplot:xlabel=' Round', ylabel='count'>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(df[df[' Value'] == 'None'].index,inplace=True)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['ValueNum'] = df[' Value'].apply(\n    lambda value: int(value.replace('$', '').replace(',','')))","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['ValueNum'].unique()\ndf['ValueNum'].nunique()\ndef binning(value):\n    if value < 1000:\n        return np.round(value, -2)\n    elif value < 10000:\n        return np.round(value, -3)\n    else:\n        return np.round(value, -4)\n\ndf['ValueBins'] = df['ValueNum'].apply(binning)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\ndef remove_tags(string):\n    result = re.sub('<.*?>','',string)\n    return result\ndf['with_out_tags']=df[' Question'].apply(lambda cw : remove_tags(cw))","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df = df[[\"with_out_tags\",\"ValueBins\"]].copy()","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df['ValueBins'] = new_df['ValueBins'].astype('category')\n# Assigning numerical values and storing in another column\nnew_df['labels'] = new_df['ValueBins'].cat.codes","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_df.labels.unique()","execution_count":11,"outputs":[{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"array([ 2,  4,  6,  8, 11, 10, 12, 14,  1,  3,  5, 13, 16,  7, 17, 15, 19,\n        9, 18,  0, 20], dtype=int8)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_df = new_df[[\"with_out_tags\",\"labels\"]].copy()\nfinal_df","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"                                            with_out_tags  labels\n0       For the last 8 years of his life, Galileo was ...       2\n1       No. 2: 1912 Olympian; football star at Carlisl...       2\n2       The city of Yuma in this state has a record av...       2\n3       In 1963, live on \"The Art Linkletter Show\", th...       2\n4       Signer of the Dec. of Indep., framer of the Co...       2\n...                                                   ...     ...\n216924  In 2006 the cast of this long-running hit emba...      11\n216925  This Puccini opera turns on the solution to 3 ...      11\n216926  In North America this term is properly applied...      11\n216927  In Penny Lane, where this \"Hellraiser\" grew up...      11\n216928  From Ft. Sill, Okla. he made the plea, Arizona...      11\n\n[213296 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>with_out_tags</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>For the last 8 years of his life, Galileo was ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>No. 2: 1912 Olympian; football star at Carlisl...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The city of Yuma in this state has a record av...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>In 1963, live on \"The Art Linkletter Show\", th...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Signer of the Dec. of Indep., framer of the Co...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>216924</th>\n      <td>In 2006 the cast of this long-running hit emba...</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>216925</th>\n      <td>This Puccini opera turns on the solution to 3 ...</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>216926</th>\n      <td>In North America this term is properly applied...</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>216927</th>\n      <td>In Penny Lane, where this \"Hellraiser\" grew up...</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>216928</th>\n      <td>From Ft. Sill, Okla. he made the plea, Arizona...</td>\n      <td>11</td>\n    </tr>\n  </tbody>\n</table>\n<p>213296 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_Df['ValueBins'].value_counts()\npossible_label = new_Df[\"ValueBins\"].unique()\nlabel_dict = {}\nfor index, possible_label in enumerate(possible_label):\n    label_dict[possible_label] = index\n\nnew_Df['label'] = new_Df[\"ValueBins\"].replace(label_dict)\nnew_Df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_Df[\"label\"].unique()\nfinal_df = new_Df[[\"with_out_tags\", \"label\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import KFold\n#new_Df = df[['with_out_tags','ValueBins']]\n#added some parameters\nkf = KFold(n_splits = 5, shuffle = True, random_state = 2)\nresult = next(kf.split(final_df), None)\n\n\n\n\ntrain = final_df.iloc[result[0]]\ntest =  final_df.iloc[result[1]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.reset_index(inplace = True, drop = True)\ntest.reset_index(inplace = True, drop = True) \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fin_df = final_df.sample(n = 50000)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(fin_df, test_size=0.2)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(train)","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"40000"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(final_df.with_out_tags.values, final_df.labels.values, test_size=0.33,random_state=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(final_df, test_size=0.2, random_state=2019)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from simpletransformers.classification import ClassificationModel, ClassificationArgs\nimport pandas as pd\nimport logging\n\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\n\nmodel_type = \"Out\"\nmodel_name = \"roberta\"\n\ntrain.columns = [\"text\", \"labels\"]\ntrain_args = {\n    'output_dir': f'{model_type}-{model_name}-outputs',\n\n    'max_seq_length': 128,\n    'num_train_epochs': 5,\n    'train_batch_size': 16,\n    'eval_batch_size': 32,\n    'gradient_accumulation_steps': 1,\n    'learning_rate': 5e-5,\n    'save_steps': -1,\n\n    'wandb_project': 'Wysa',\n    'wandb_kwargs': {'name': f'{model_type}-{model_name}'},\n    'evaluate_during_training': True,\n    'evaluate_during_training_steps': 1000,\n    'reprocess_input_data': True,\n    \"save_model_every_epoch\": False,\n    'overwrite_output_dir': True,\n    'no_cache': True,\n    'no_save' : True,\n\n    'use_early_stopping': True,\n    'early_stopping_patience': 3,\n    'manual_seed': 4,\n}\n\n\n\n\ntest.columns = [\"text\", \"labels\"]\n\n\n# Optional model configuration\n#model_args = ClassificationArgs(num_train_epochs=8)\n\n# Create a ClassificationModel\nmodel = ClassificationModel(\n'roberta', 'roberta-base',\n    num_labels=20,\n    #args=model_args,\n    args=train_args\n) \n\n#model = ClassificationModel(model_type, model_name, num_labels=4, args=train_args)\n# Train the model\nmodel.train_model(train,eval_df=test)\n\n# Evaluate the model\n#result, model_outputs, wrong_predictions = model.eval_model(test)\n\n# Make predictions with the model\n#predictions, raw_outputs = model.predict([\"Sam was a Wizard\"])\n","execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e7a48472fdc4cddac83990347db4f9e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5277cd5e65bb4dac9f019d7a0ddf76b2"}},"metadata":{}},{"output_type":"stream","text":"Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b83f12e4ae34b569d418fe9a2ddab8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fe5fca38c3f4dd0a24eb73642f73252"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4337f75eba744a3970bffba97ae3721"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d4567dce14c46a2b7fe86a4938c7366"}},"metadata":{}},{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","name":"stderr"},{"output_type":"stream","name":"stdout","text":"wandb: Paste an API key from your profile and hit enter: ········\n"},{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Tracking run with wandb version 0.10.18<br/>\n                Syncing run <strong style=\"color:#cdcd00\">Out-roberta</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/devanshchoubey/Wysa\" target=\"_blank\">https://wandb.ai/devanshchoubey/Wysa</a><br/>\n                Run page: <a href=\"https://wandb.ai/devanshchoubey/Wysa/runs/35lfe8n7\" target=\"_blank\">https://wandb.ai/devanshchoubey/Wysa/runs/35lfe8n7</a><br/>\n                Run data is saved locally in <code>/kaggle/working/wandb/run-20210317_183256-35lfe8n7</code><br/><br/>\n            "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 0 of 5:   0%|          | 0/2500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"597f74fa6bcf498aabc26f506018343e"}},"metadata":{}},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:846: RuntimeWarning: invalid value encountered in double_scalars\n  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"Running Epoch 1 of 5:   0%|          | 0/2500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"828d3cca381f4139bbc1200ffb33ea6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Running Epoch 2 of 5:   0%|          | 0/2500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf3039e7635f4f669106b401426f8f9d"}},"metadata":{}},{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"(6000,\n {'global_step': [1000, 2000, 2500, 3000, 4000, 5000, 5000, 6000],\n  'mcc': [0.005503259743957464, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n  'train_loss': [1.9665800333023071,\n   2.2189440727233887,\n   2.1500139236450195,\n   2.266141414642334,\n   2.181302547454834,\n   2.6613011360168457,\n   2.6613011360168457,\n   2.086963176727295],\n  'eval_loss': [2.1768621050130825,\n   2.1539169866056107,\n   2.135093469208422,\n   2.1392436766395933,\n   2.1460866806225276,\n   2.144215387277329,\n   2.144215387277329,\n   2.1405205997034384]})"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"du - h ./final-roberta-outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"du -h final-roberta-outputs/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls -ltr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_best = ClassificationModel('roberta', 'outputs/best_model/', num_labels=5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score, accuracy_score\n\n\ndef f1_multiclass(labels, preds):\n    return f1_score(labels, preds, average='micro')\n    \n#result, model_outputs, wrong_predictions = model.eval_model(eval_df, f1=f1_multiclass, acc=accuracy_score)\n\nresult, model_outputs, wrong_predictions = model.eval_model(test, f1=f1_multiclass, acc=accuracy_score)","execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"Running Evaluation:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72097d7ba36042c580d33d5285ecc0ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:35lfe8n7) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br/>Waiting for W&B process to finish, PID 406<br/>Program ended successfully."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value=' 0.03MB of 0.03MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d22628fad9f94f4aad411b31b5c7f944"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find user logs for this run at: <code>/kaggle/working/wandb/run-20210317_183256-35lfe8n7/logs/debug.log</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find internal logs for this run at: <code>/kaggle/working/wandb/run-20210317_183256-35lfe8n7/logs/debug-internal.log</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>Training loss</td><td>2.08696</td></tr><tr><td>lr</td><td>3e-05</td></tr><tr><td>global_step</td><td>6000</td></tr><tr><td>_runtime</td><td>1980</td></tr><tr><td>_timestamp</td><td>1616007956</td></tr><tr><td>_step</td><td>127</td></tr><tr><td>mcc</td><td>0.0</td></tr><tr><td>train_loss</td><td>2.08696</td></tr><tr><td>eval_loss</td><td>2.14052</td></tr></table>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>Training loss</td><td>█▃▂▃▂▆▁▄▂▂▃▄▃▃▂▂▃▄▂▅▅▂▃▂▃▃▃▃▂▁▄▃▄▃▄▃▃▄▃▂</td></tr><tr><td>lr</td><td>▁▃▄▆▇██████▇▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▆▆▆▅▅▅▅▅▅▅▅▅</td></tr><tr><td>global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>mcc</td><td>█▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>▁▄▃▄▃██▂</td></tr><tr><td>eval_loss</td><td>█▄▁▂▃▃▃▂</td></tr></table><br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                    <br/>Synced <strong style=\"color:#cdcd00\">Out-roberta</strong>: <a href=\"https://wandb.ai/devanshchoubey/Wysa/runs/35lfe8n7\" target=\"_blank\">https://wandb.ai/devanshchoubey/Wysa/runs/35lfe8n7</a><br/>\n                "},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"...Successfully finished last run (ID:35lfe8n7). Initializing new run:<br/><br/>"},"metadata":{}},{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.22 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Tracking run with wandb version 0.10.18<br/>\n                Syncing run <strong style=\"color:#cdcd00\">Out-roberta</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/devanshchoubey/Wysa\" target=\"_blank\">https://wandb.ai/devanshchoubey/Wysa</a><br/>\n                Run page: <a href=\"https://wandb.ai/devanshchoubey/Wysa/runs/66zx9k6m\" target=\"_blank\">https://wandb.ai/devanshchoubey/Wysa/runs/66zx9k6m</a><br/>\n                Run data is saved locally in <code>/kaggle/working/wandb/run-20210317_192050-66zx9k6m</code><br/><br/>\n            "},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"result","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"{'mcc': 0.0, 'f1': 0.1426, 'acc': 0.1426, 'eval_loss': 2.1405205997034384}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":24,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"<simpletransformers.classification.classification_utils.InputExample at 0x7f6aed6a9e50>"},"metadata":{}}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install simpletransformers","execution_count":16,"outputs":[{"output_type":"stream","text":"Collecting simpletransformers\n  Downloading simpletransformers-0.60.9-py3-none-any.whl (206 kB)\n\u001b[K     |████████████████████████████████| 206 kB 1.3 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: tqdm>=4.47.0 in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (4.55.1)\nCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[K     |████████████████████████████████| 43 kB 1.3 MB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (2.25.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (1.1.5)\nCollecting streamlit\n  Downloading streamlit-0.78.0-py2.py3-none-any.whl (7.5 MB)\n\u001b[K     |████████████████████████████████| 7.5 MB 7.5 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (1.19.5)\nRequirement already satisfied: tokenizers in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (0.9.4)\nRequirement already satisfied: tensorboardx in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (2.1)\nRequirement already satisfied: regex in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (2020.11.13)\nRequirement already satisfied: transformers>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (4.2.2)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (0.10.18)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (0.1.95)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (0.23.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from simpletransformers) (1.4.1)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers>=4.2.0->simpletransformers) (0.0.43)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers>=4.2.0->simpletransformers) (20.8)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers>=4.2.0->simpletransformers) (3.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers>=4.2.0->simpletransformers) (3.0.12)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers>=4.2.0->simpletransformers) (3.4.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers>=4.2.0->simpletransformers) (3.7.4.3)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers>=4.2.0->simpletransformers) (2.4.7)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->simpletransformers) (2.8.1)\nRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->simpletransformers) (2020.5)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->simpletransformers) (1.15.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->simpletransformers) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->simpletransformers) (2020.12.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->simpletransformers) (1.26.2)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->simpletransformers) (3.0.4)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=4.2.0->simpletransformers) (7.1.2)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers>=4.2.0->simpletransformers) (1.0.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->simpletransformers) (2.1.0)\nRequirement already satisfied: cachetools>=4.0 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (4.1.1)\nCollecting watchdog\n  Downloading watchdog-2.0.2-py3-none-manylinux2014_x86_64.whl (74 kB)\n\u001b[K     |████████████████████████████████| 74 kB 2.5 MB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: gitpython in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (3.1.12)\nRequirement already satisfied: altair>=3.2.0 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (4.1.0)\nRequirement already satisfied: protobuf!=3.11,>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (3.14.0)\nRequirement already satisfied: toml in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (0.10.2)\nCollecting astor\n  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\nCollecting validators\n  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (7.2.0)\nRequirement already satisfied: tornado>=5.0 in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (5.0.2)\nCollecting pydeck>=0.1.dev5\n  Downloading pydeck-0.6.1-py2.py3-none-any.whl (4.6 MB)\n\u001b[K     |████████████████████████████████| 4.6 MB 17.7 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: pyarrow in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (1.0.1)\nRequirement already satisfied: blinker in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (1.4)\nRequirement already satisfied: tzlocal in /opt/conda/lib/python3.7/site-packages (from streamlit->simpletransformers) (2.1)\nCollecting base58\n  Downloading base58-2.1.0-py3-none-any.whl (5.6 kB)\nRequirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from altair>=3.2.0->streamlit->simpletransformers) (2.11.2)\nRequirement already satisfied: jsonschema in /opt/conda/lib/python3.7/site-packages (from altair>=3.2.0->streamlit->simpletransformers) (3.2.0)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.7/site-packages (from altair>=3.2.0->streamlit->simpletransformers) (0.11.1)\nCollecting ipykernel>=5.1.2\n  Downloading ipykernel-5.5.0-py3-none-any.whl (120 kB)\n\u001b[K     |████████████████████████████████| 120 kB 29.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: traitlets>=4.3.2 in /opt/conda/lib/python3.7/site-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (5.0.5)\nRequirement already satisfied: ipywidgets>=7.0.0 in /opt/conda/lib/python3.7/site-packages (from pydeck>=0.1.dev5->streamlit->simpletransformers) (7.6.2)\nRequirement already satisfied: jupyter-client in /opt/conda/lib/python3.7/site-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (6.1.7)\nRequirement already satisfied: ipython>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (7.19.0)\nRequirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\nRequirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.8.0)\nRequirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (49.6.0.post20201009)\nRequirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.5)\nRequirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (2.7.3)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.0.8)\nRequirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.4.2)\nRequirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.17.2)\nRequirement already satisfied: widgetsnbextension~=3.5.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.5.1)\nRequirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.0.8)\nRequirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.0.0)\n","name":"stdout"},{"output_type":"stream","text":"Requirement already satisfied: parso<0.8.0,>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.1)\nRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2->altair>=3.2.0->streamlit->simpletransformers) (1.1.1)\nRequirement already satisfied: jupyter-core in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (4.7.0)\nRequirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.0)\nRequirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->altair>=3.2.0->streamlit->simpletransformers) (0.17.3)\nRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->altair>=3.2.0->streamlit->simpletransformers) (20.3.0)\nRequirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.7.0)\nRequirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.0.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.2.5)\nRequirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (5.5.0)\nRequirement already satisfied: Send2Trash in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.5.0)\nRequirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (20.0.0)\nRequirement already satisfied: terminado>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.9.2)\nRequirement already satisfied: nbconvert in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (6.0.7)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from gitpython->streamlit->simpletransformers) (4.0.5)\nRequirement already satisfied: smmap<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->gitpython->streamlit->simpletransformers) (3.0.4)\nRequirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.1.2)\nRequirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.1)\nRequirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.8.4)\nRequirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.4.4)\nRequirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.6.0)\nRequirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.4.2)\nRequirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (3.2.1)\nRequirement already satisfied: nest-asyncio in /opt/conda/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.4.3)\nRequirement already satisfied: async-generator in /opt/conda/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (1.10)\nRequirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit->simpletransformers) (0.5.1)\nRequirement already satisfied: configparser>=3.8.1 in /opt/conda/lib/python3.7/site-packages (from wandb->simpletransformers) (5.0.1)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb->simpletransformers) (2.3)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb->simpletransformers) (0.4.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb->simpletransformers) (0.1.2)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb->simpletransformers) (5.8.0)\nRequirement already satisfied: subprocess32>=3.5.3 in /opt/conda/lib/python3.7/site-packages (from wandb->simpletransformers) (3.5.4)\nRequirement already satisfied: sentry-sdk>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb->simpletransformers) (0.19.5)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb->simpletransformers) (5.3.1)\nRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb->simpletransformers) (1.0.1)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16170 sha256=b279f307262b9019bb221c9d16e3a0ea2211f6d0816a37f8934fb9d216f33b31\n  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\nSuccessfully built seqeval\nInstalling collected packages: ipykernel, watchdog, validators, pydeck, base58, astor, streamlit, seqeval, simpletransformers\n  Attempting uninstall: ipykernel\n    Found existing installation: ipykernel 5.1.1\n    Uninstalling ipykernel-5.1.1:\n      Successfully uninstalled ipykernel-5.1.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\njupyterlab-git 0.11.0 requires nbdime<2.0.0,>=1.1.0, but you have nbdime 2.1.0 which is incompatible.\u001b[0m\nSuccessfully installed astor-0.8.1 base58-2.1.0 ipykernel-5.5.0 pydeck-0.6.1 seqeval-1.2.2 simpletransformers-0.60.9 streamlit-0.78.0 validators-0.18.2 watchdog-2.0.2\n","name":"stdout"}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!wget https://s3.amazonaws.com/fast-ai-nlp/ag_news_csv.tgz","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report","execution_count":42,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bow = CountVectorizer(stop_words='english', max_features=2000)\nbow.fit(train['text'])","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"CountVectorizer(max_features=2000, stop_words='english')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = bow.transform(train[\"text\"])\nX_test = bow.transform(test[\"text\"])\n\ny_train = train.labels\ny_test = test.labels\n\nprint(\"Shape of X_train:\", X_train.shape)\nprint(\"Shape of X_test:\", X_test.shape)\nprint(\"Shape of y_train:\", y_train.shape)\nprint(\"Shape of y_test:\", y_test.shape)","execution_count":38,"outputs":[{"output_type":"stream","text":"Shape of X_train: (40000, 2000)\nShape of X_test: (10000, 2000)\nShape of y_train: (40000,)\nShape of y_test: (10000,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = LogisticRegression(solver='saga', multi_class='multinomial', max_iter=200)\nlr.fit(X_train, y_train)","execution_count":40,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n  \"the coef_ did not converge\", ConvergenceWarning)\n","name":"stderr"},{"output_type":"execute_result","execution_count":40,"data":{"text/plain":"LogisticRegression(max_iter=200, multi_class='multinomial', solver='saga')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = lr.predict(X_test)\n\nprint(classification_report(y_test, y_pred))","execution_count":44,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00         1\n           1       0.08      0.01      0.02       429\n           2       0.16      0.16      0.16      1403\n           3       0.06      0.01      0.02       409\n           4       0.20      0.40      0.27      1986\n           5       0.11      0.02      0.03       472\n           6       0.11      0.05      0.07       993\n           7       0.00      0.00      0.00         7\n           8       0.14      0.13      0.13      1426\n           9       0.00      0.00      0.00         3\n          10       0.18      0.20      0.19      1561\n          11       0.16      0.11      0.13      1202\n          12       0.00      0.00      0.00        38\n          13       0.00      0.00      0.00        38\n          14       0.00      0.00      0.00        22\n          15       0.00      0.00      0.00         4\n          16       0.00      0.00      0.00         2\n          17       0.00      0.00      0.00         2\n          18       0.00      0.00      0.00         1\n          19       0.00      0.00      0.00         1\n\n    accuracy                           0.17     10000\n   macro avg       0.06      0.05      0.05     10000\nweighted avg       0.15      0.17      0.15     10000\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"DONE\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}